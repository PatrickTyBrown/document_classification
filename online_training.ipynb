{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for N Batches with Dynamically Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import string\n",
    "import random\n",
    "import albumentations as A\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Augmentation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_field(length = 0):\n",
    "    length = length if length else random.randint(2, 20)\n",
    "    field = ''.join(random.choices(string.ascii_letters + string.digits + string.punctuation, k=length))\n",
    "    return field\n",
    "\n",
    "transform = A.Compose([\n",
    "        A.ImageCompression(quality_lower=10, p=0.1),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(p=0.8),\n",
    "            A.ISONoise(p=0.2),\n",
    "            A.MultiplicativeNoise(p=.05)\n",
    "        ], p=0.1),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=0.05),\n",
    "        A.ShiftScaleRotate(shift_limit=0.15, scale_limit=0.2, rotate_limit=30, p=0.5),\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.5),\n",
    "            A.GridDistortion(p=.5),\n",
    "            A.PiecewiseAffine(p=0.5),\n",
    "        ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.CLAHE(clip_limit=2),\n",
    "            A.Sharpen(),\n",
    "            A.Emboss(),\n",
    "            A.RandomBrightnessContrast(),            \n",
    "        ], p=0.05),\n",
    "        A.OneOf([\n",
    "            A.RandomFog(),\n",
    "            A.RandomRain(),\n",
    "            A.RandomSnow(),\n",
    "            A.RandomSunFlare(),            \n",
    "        ], p=0.1),\n",
    "        A.HueSaturationValue(p=0.01)\n",
    "    ])\n",
    "\n",
    "fonts = [\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_PLAIN,\n",
    "    cv2.FONT_HERSHEY_DUPLEX,\n",
    "    cv2.FONT_HERSHEY_TRIPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_ITALIC]\n",
    "\n",
    "def generate_target_dictionary():\n",
    "    with open('data_dictionary.json') as data_dict:\n",
    "        categories = json.load(data_dict)['target_data']\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables we will need for augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_directory = 'templates_img'\n",
    "text_locations = json.load(open('text_locations.json', 'r'))\n",
    "image_dir =  \"data/\"\n",
    "backgrounds_dir = 'image_backgrounds'\n",
    "\n",
    "categories = generate_target_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models():\n",
    "    models={}\n",
    "    nb_full = BernoulliNB(alpha=10)\n",
    "    for key in categories.keys():\n",
    "        models[key] =  BernoulliNB(alpha=0.1)\n",
    "    return models, nb_full\n",
    "\n",
    "def fit_on_batch(x,y, models, nb_full):\n",
    "    nb_full.partial_fit(x, y, classes=[0,1,2,3,4,5,6,7])\n",
    "    for key in categories.keys():\n",
    "        target_class = categories[key]\n",
    "        models[key].partial_fit(x, y==target_class, classes=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_batch(x,y, models, nb_full, batch):\n",
    "\n",
    "    results = np.zeros((y.shape[0],8))\n",
    "    nb_full_results = np.zeros((y.shape[0],8))\n",
    "\n",
    "    y_pred = nb_full.predict_proba(x)[:]\n",
    "    nb_full_results = y_pred\n",
    "    for key in categories.keys():\n",
    "        y_pred = models[key].predict_proba(x)[:,0]\n",
    "        results[:,categories[key]] = y_pred\n",
    "\n",
    "    test = np.copy(y)\n",
    "    test2 = np.copy(y)\n",
    "    test3 = np.copy(y)\n",
    "\n",
    "    added_results = np.zeros(nb_full_results.shape)\n",
    "\n",
    "    for  i in range(results[:,0].shape[0]):\n",
    "        added_results[i,:] = results[i,:] + (np.absolute(nb_full_results[i,:]-1)/1.0e+200)\n",
    "        max = np.where(results[i,:] == np.amin(results[i,:].reshape(8)))[0]\n",
    "        max2 = np.where(added_results[i,:] == np.amin(added_results[i,:].reshape(8)))[0]\n",
    "        max3 = np.where(nb_full_results[i,:]== np.amax(nb_full_results[i,:].reshape(8)))[0]\n",
    "        if len(max)>1:\n",
    "            test[i] = 5\n",
    "        else:\n",
    "            test[i]=max[0]\n",
    "\n",
    "        if len(max2)>1:\n",
    "            test2[i] = 5\n",
    "        else:\n",
    "            test2[i]=max2[0]\n",
    "\n",
    "        if len(max3)>1:\n",
    "            test3[i] = 5\n",
    "        else:\n",
    "            test3[i]=max3[0]\n",
    "\n",
    "    with open(\"training_log.txt\", \"a\") as file:\n",
    "        \n",
    "        file.writelines('\\n\\n\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(f'\\t\\t\\t\\t\\tIteration:{batch}')\n",
    "\n",
    "\n",
    "        y_pred = test\n",
    "        mask = y_pred != 5\n",
    "\n",
    "        file.writelines('\\n\\nBase OVR Ensemble')\n",
    "        file.writelines(f'\\n\\nData Size:\\t{len(y_pred)}/{len(y_pred)}')\n",
    "        file.writelines(f'\\n\\tAccuracy:\\t{accuracy_score(y, y_pred)}')\n",
    "        file.writelines(f'\\n\\tPrecision:\\t{precision_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tRecall:\\t{recall_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tF1:\\t{f1_score(y, y_pred, average=\"macro\")}\\n')\n",
    "        file.writelines(str(confusion_matrix(y, y_pred)))\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        file.writelines('\\n\\nFiltered OVR Ensemble')\n",
    "        file.writelines(f'\\n\\nData Size:\\t{len(y_pred[mask])}/{len(y_pred)}')\n",
    "        file.writelines(f'\\n\\tAccuracy:\\t{accuracy_score(y[mask], y_pred[mask])}')\n",
    "        file.writelines(f'\\n\\tPrecision:\\t{precision_score(y[mask], y_pred[mask], average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tRecall:\\t{recall_score(y[mask], y_pred[mask], average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tF1:\\t{f1_score(y[mask], y_pred[mask], average=\"macro\")}\\n')\n",
    "        file.writelines(str(confusion_matrix(y[mask], y_pred[mask])))\n",
    "        \n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = test3\n",
    "        file.writelines('\\n\\nBase Multiclass')\n",
    "        file.writelines(f'\\n\\nData Size:\\t{len(y_pred)}/{len(y_pred)}')\n",
    "        file.writelines(f'\\n\\tAccuracy:\\t{accuracy_score(y, y_pred)}')\n",
    "        file.writelines(f'\\n\\tPrecision:\\t{precision_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tRecall:\\t{recall_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tF1:\\t{f1_score(y, y_pred, average=\"macro\")}\\n')\n",
    "        file.writelines(str(confusion_matrix(y, y_pred)))\n",
    "\n",
    "def checkpoint(models, nb_full, iteration):\n",
    "    with open(f'models/EnsembleModels_{iteration}', 'ab') as file:\n",
    "        pickle.dump(models, file)       \n",
    "\n",
    "    with open(f'models/MulticlassModel_{iteration}', 'ab') as file:\n",
    "        pickle.dump(nb_full, file)                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_backgrounds():\n",
    "    for filename in os.listdir(backgrounds_dir):\n",
    "        backgrounds = []\n",
    "        img = cv2.imread(backgrounds_dir+ '/'+filename, 1)\n",
    "        backgrounds.append(img)\n",
    "    return backgrounds\n",
    "\n",
    "\n",
    "\n",
    "def agument_image(image, backgrounds):\n",
    "    background_img = backgrounds[random.randint(0, len(backgrounds))-1]\n",
    "    img = image\n",
    "    for loc in doc_info:\n",
    "        font = random.choice(fonts)\n",
    "        cv2.putText(img, generate_random_field(),\n",
    "                    (int(loc['x']),int(loc['y'])), font,\n",
    "                    1, (0, 0, 0), 1)\n",
    "    # resize(210,275)\n",
    "    x_size = random.randint(-150,400)\n",
    "    y_size = random.randint(-150,400)\n",
    "    x_size = x_size if x_size > 150 else 0\n",
    "    y_size = y_size if y_size > 150 else 0\n",
    "    x_offset = int(x_size/1.5)\n",
    "    y_offset = int(y_size/1.5)\n",
    "    background_img = cv2.resize(background_img, (850+abs(x_size), 1100+abs(y_size))) \n",
    "\n",
    "    background_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    transformed = transform(image=cv2.resize(background_img, (200,200)))\n",
    "    img = transformed['image']\n",
    "\n",
    "    # img = cv2.resize(img, (100,100))\n",
    "    # img = cv2.resize(img, (200,200))\n",
    "\n",
    "    # img = cv2.resize(img, (850+abs(x_size), 1100+abs(y_size)))\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    return img\n",
    "\n",
    "\n",
    "for filename in text_locations:\n",
    "    if text_locations[filename] != {}:\n",
    "        image = cv2.imread(template_directory+ '/'+filename, 1)\n",
    "        image = cv2.resize(image, (850, 1100)) \n",
    "\n",
    "        doc_info = text_locations[filename]\n",
    "        for i in range(3):\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(batch_size, num_batches, test_frequency, models=False, nb_full=False):\n",
    "    backgrounds = load_backgrounds()\n",
    "    chunk = batch_size//8\n",
    "    \n",
    "    if not models:\n",
    "        models, nb_full = generate_models()\n",
    "    \n",
    "    for iteration in range(num_batches):\n",
    "        print(f'\\nIteration:\\t{iteration}')\n",
    "        x = np.zeros(shape=(batch_size,200*200))\n",
    "        y = np.zeros((batch_size,))\n",
    "        index = 0\n",
    "        #generate our data\n",
    "        for filename in text_locations:\n",
    "            \n",
    "            if text_locations[filename] != {}:\n",
    "                image = cv2.imread(template_directory+ '/'+filename, 1)\n",
    "                image = cv2.resize(image, (850, 1100)) \n",
    "\n",
    "                doc_info = text_locations[filename]\n",
    "                for row in range(chunk):\n",
    "                    print(index,end='\\r', flush=True)\n",
    "                    img = agument_image(image, backgrounds)\n",
    "                    x[index] =  np.reshape(img, (200*200))\n",
    "                    y[index] = categories[filename[:4]]\n",
    "                    index = index + 1\n",
    "        \n",
    "\n",
    "        if iteration%test_frequency == 0 and iteration!= 0:\n",
    "            test_on_batch(x, y, models, nb_full, iteration)\n",
    "            checkpoint(models, nb_full, iteration)\n",
    "        else:\n",
    "            fit_on_batch(x, y, models, nb_full)\n",
    "    return models, nb_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:\t0\n",
      "6012\r"
     ]
    }
   ],
   "source": [
    "models, nb_full = main(batch_size=8000, num_batches=7, test_frequency=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Inco': BernoulliNB(alpha=0.1),\n",
       " 'Teac': BernoulliNB(alpha=0.1),\n",
       " 'Cons': BernoulliNB(alpha=0.1),\n",
       " 'Publ': BernoulliNB(alpha=0.1),\n",
       " 'Econ': BernoulliNB(alpha=0.1),\n",
       " 'TeaF': BernoulliNB(alpha=0.1),\n",
       " 'Gene': BernoulliNB(alpha=0.1),\n",
       " 'Reaf': BernoulliNB(alpha=0.1)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.47265635, -3.47265635, -3.41858912, ..., -3.41858912,\n",
       "        -3.36729583, -3.47265635],\n",
       "       [-0.69314718, -0.69314718, -0.69314718, ..., -0.69314718,\n",
       "        -0.69314718, -0.69314718],\n",
       "       [-0.69314718, -0.69314718, -0.69314718, ..., -0.69314718,\n",
       "        -0.69314718, -0.69314718],\n",
       "       ...,\n",
       "       [-0.69314718, -0.69314718, -0.69314718, ..., -0.69314718,\n",
       "        -0.69314718, -0.69314718],\n",
       "       [-0.69314718, -0.69314718, -0.69314718, ..., -0.69314718,\n",
       "        -0.69314718, -0.69314718],\n",
       "       [-0.69314718, -0.69314718, -0.69314718, ..., -0.69314718,\n",
       "        -0.69314718, -0.69314718]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0e1bddb7ed37f67f576e4a6a21b2408ebab9a006050dc6d038d31036c144c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
