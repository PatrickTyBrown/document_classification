{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Style Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import string\n",
    "import random\n",
    "import albumentations as A\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_field(length = 0):\n",
    "    length = length if length else random.randint(2, 20)\n",
    "    field = ''.join(random.choices(string.ascii_letters + string.digits + string.punctuation, k=length))\n",
    "    return field\n",
    "\n",
    "transform = A.Compose([\n",
    "        # A.RandomRotate90(),\n",
    "        # A.Flip(),\n",
    "        # A.Transpose(),\n",
    "        A.ImageCompression(quality_lower=5, p=0.1),\n",
    "        A.OneOf([\n",
    "            # A.IAAAdditiveGaussianNoise(),\n",
    "            A.GaussNoise(p=0.8,var_limit=(0,25)),\n",
    "            # A.ISONoise(p=0.2,),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=1, p=0.1),\n",
    "            A.Blur(blur_limit=1, p=0.1),\n",
    "        ], p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.25, rotate_limit=15, p=0.25),\n",
    "        # A.OneOf([\n",
    "        #     A.OpticalDistortion(p=0.5),\n",
    "        #     A.GridDistortion(p=.5),\n",
    "        #     A.PiecewiseAffine(p=0.5),\n",
    "        # ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.RandomFog(),\n",
    "            A.RandomRain(),\n",
    "            A.RandomSnow(),\n",
    "            A.RandomSunFlare(),            \n",
    "        ], p=0.1),\n",
    "        # A.HueSaturationValue(p=0.01),\n",
    "        # A.Rotate(66,p=0.3)\n",
    "        # A.ToGray(always_apply=True)\n",
    "    ])\n",
    "# random.seed(42) \n",
    "\n",
    "fonts = [\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_PLAIN,\n",
    "    cv2.FONT_HERSHEY_DUPLEX,\n",
    "    cv2.FONT_HERSHEY_TRIPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_ITALIC]\n",
    "\n",
    "def generate_target_dictionary():\n",
    "    with open('data_dictionary.json') as data_dict:\n",
    "        categories = json.load(data_dict)['target_data']\n",
    "    return categories\n",
    "\n",
    "template_directory = 'templates_img'\n",
    "text_locations = json.load(open('text_locations.json', 'r'))\n",
    "image_dir =  \"data/\"\n",
    "backgrounds_dir = 'image_backgrounds'\n",
    "\n",
    "categories = generate_target_dictionary()\n",
    "\n",
    "def load_backgrounds():\n",
    "    for filename in os.listdir(backgrounds_dir):\n",
    "        backgrounds = []\n",
    "        img = cv2.imread(backgrounds_dir+ '/'+filename, 1)\n",
    "        backgrounds.append(img)\n",
    "    return backgrounds\n",
    "\n",
    "\n",
    "\n",
    "def agument_image(image, backgrounds, doc_info):\n",
    "    background_img = backgrounds[random.randint(0, len(backgrounds))-1]\n",
    "    img = image\n",
    "    for loc in doc_info:\n",
    "        font = random.choice(fonts)\n",
    "        cv2.putText(img, generate_random_field(),\n",
    "                    (int(loc['x']),int(loc['y'])), font,\n",
    "                    1, (0, 0, 0), 1)\n",
    "\n",
    "    x_size = random.randint(-150,400)\n",
    "    y_size = random.randint(-150,400)\n",
    "    x_size = x_size if x_size > 150 else 0\n",
    "    y_size = y_size if y_size > 150 else 0\n",
    "    x_offset = int(x_size/1.5)\n",
    "    y_offset = int(y_size/1.5)\n",
    "    background_img = cv2.resize(background_img, (850+abs(x_size), 1100+abs(y_size))) \n",
    "\n",
    "    background_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    transformed = transform(image=cv2.resize(background_img,(460,720)))\n",
    "    img = transformed['image']\n",
    "\n",
    "    img_280 =cv2.resize(img,(280,360))\n",
    "    img_200 =cv2.resize(img,(200,200))\n",
    "    img_85 =cv2.resize(img,(85,110))\n",
    "    img_42 = cv2.resize(img,(42,65))\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_280 = cv2.adaptiveThreshold(cv2.cvtColor(img_280, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    img_200 = cv2.adaptiveThreshold(cv2.cvtColor(img_200, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    img_85 = cv2.adaptiveThreshold(cv2.cvtColor(img_85, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    img_42 = cv2.adaptiveThreshold(cv2.cvtColor(img_42, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    return img_280/255, img_200/255, img_85/255, img_42/255\n",
    "\n",
    "dim_size = (280*360)\n",
    "dim_shape = (280,360)\n",
    "\n",
    "def generate_dataset(batch_size):\n",
    "    backgrounds = load_backgrounds()\n",
    "    chunk = batch_size//6\n",
    "\n",
    "    x_280 = np.zeros(shape=(batch_size,(280*360)))\n",
    "    x_200 = np.zeros(shape=(batch_size,(200*200)))\n",
    "    x_85 = np.zeros(shape=(batch_size,(85*110)))\n",
    "    x_42 = np.zeros(shape=(batch_size,(42*65)))\n",
    "\n",
    "    y = np.zeros((batch_size,))\n",
    "    index = 0\n",
    "    #generate our data\n",
    "    for filename in text_locations:\n",
    "        \n",
    "        if text_locations[filename] != {}:\n",
    "            image = cv2.imread(template_directory+ '/'+filename, 1)\n",
    "            image = cv2.resize(image, (850, 1100)) \n",
    "\n",
    "            doc_info = text_locations[filename]\n",
    "            for row in range(chunk):\n",
    "                print(index,end='\\r', flush=True)\n",
    "                img_280, img_200, img_85, img_42 = agument_image(image, backgrounds, doc_info)\n",
    "                x_280[index] =  np.reshape(img_280, (280*360))\n",
    "                x_200[index] =  np.reshape(img_200, (200*200))\n",
    "                x_85[index] =  np.reshape(img_85, (85*110))\n",
    "                x_42[index] =  np.reshape(img_42, (42*65))\n",
    "                y[index] = categories[filename[:4]]\n",
    "                index = index + 1\n",
    "\n",
    "    return x_280, x_200, x_85, x_42, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_batch(x_280, x_200, x_85, x_42, y):\n",
    "    # Models\n",
    "    #nb_full_280\n",
    "    # models_280\n",
    "    # nb_full_200\n",
    "    # models_200\n",
    "    # nb_full_42\n",
    "    # models_42\n",
    "\n",
    "    def write_results(y, y_pred, heading):\n",
    "        file.writelines(f'\\n\\n{heading}')\n",
    "        file.writelines(f'\\n\\nData Size:\\t{len(y_pred)}/{len(y_pred)}')\n",
    "        file.writelines(f'\\n\\tAccuracy:\\t{accuracy_score(y, y_pred)}')\n",
    "        file.writelines(f'\\n\\tPrecision:\\t{precision_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tRecall:\\t{recall_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tF1:\\t{f1_score(y, y_pred, average=\"macro\")}\\n')\n",
    "        file.writelines(str(confusion_matrix(y, y_pred)))\n",
    "        return True\n",
    "\n",
    "    OVR_results = np.zeros((4,y.shape[0],6))\n",
    "    Multi_results = np.zeros((4, y.shape[0],6))\n",
    "\n",
    "    Multi_results[0] = nb_full_280.predict_proba(x_280)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[0,:,categories[key]] = models_280[key].predict_proba(x_280)[:,0]\n",
    "\n",
    "    Multi_results[1] = nb_full_200.predict_proba(x_200)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[1,:,categories[key]] = models_200[key].predict_proba(x_200)[:,0]\n",
    "\n",
    "    Multi_results[2] = nb_full_85.predict_proba(x_85)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[2,:,categories[key]] = models_85[key].predict_proba(x_85)[:,0]\n",
    "\n",
    "    Multi_results[3] = nb_full_42.predict_proba(x_42)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[3,:,categories[key]] = models_42[key].predict_proba(x_42)[:,0]\n",
    "\n",
    "    test = np.copy(y)\n",
    "    test2 = np.copy(y)\n",
    "    test3 = np.copy(y)\n",
    "\n",
    "    nb_full_280_choice = np.copy(y)\n",
    "    models_280_choice = np.copy(y)\n",
    "    nb_full_200_choice = np.copy(y)\n",
    "    models_200_choice = np.copy(y)\n",
    "    nb_full_85_choice = np.copy(y)\n",
    "    models_85_choice = np.copy(y)\n",
    "    nb_full_42_choice = np.copy(y)\n",
    "    models_42_choice = np.copy(y)\n",
    "    voting_sum = np.array((y.shape[0],6))\n",
    "    voting_results = np.copy(y)\n",
    "\n",
    "    for  i in range(y.shape[0]):\n",
    "        # print(i)\n",
    "        # added_results[i,:] = results[i,:] + (np.absolute(nb_full_results[i,:]-1)/1.0e+200)\n",
    "        nb_full_280_choice[i] = np.where(Multi_results[0, i,:] == np.amax(Multi_results[0,i,:].reshape(6)))[0]\n",
    "        models_280_choice_ = np.where(OVR_results[0, i,:] == np.amin(OVR_results[0,i,:].reshape(6)))[0]\n",
    "        nb_full_200_choice[i] = np.where(Multi_results[1, i,:] == np.amax(Multi_results[1,i,:].reshape(6)))[0]\n",
    "        models_200_choice_ = np.where(OVR_results[1, i,:] == np.amin(OVR_results[1,i,:].reshape(6)))[0]\n",
    "        nb_full_85_choice[i] = np.where(Multi_results[2, i,:] == np.amax(Multi_results[2,i,:].reshape(6)))[0]\n",
    "        models_85_choice_ = np.where(OVR_results[2, i,:] == np.amin(OVR_results[2,i,:].reshape(6)))[0]\n",
    "        nb_full_42_choice[i] = np.where(Multi_results[3, i,:] == np.amax(Multi_results[3,i,:].reshape(6)))[0]\n",
    "        models_42_choice_ = np.where(OVR_results[3, i,:] == np.amin(OVR_results[3,i,:].reshape(6)))[0]\n",
    "\n",
    "        # print(Multi_results[:,i,:])\n",
    "        # print(np.sum(Multi_results[:,i,:], axis=1))\n",
    "        # print(np.sum(Multi_results[:,i,:], axis=0))\n",
    "        Multi_results[0,i,:] *= 8\n",
    "        Multi_results[1,i,:] *= 6\n",
    "        Multi_results[2,i,:] *= 4\n",
    "        Multi_results[3,i,:] *= 2\n",
    "        OVR_results[0,i,:] = np.abs((OVR_results[0,i,:] -1)*6)\n",
    "        OVR_results[1,i,:] = np.abs((OVR_results[1,i,:] -1)*1.5)\n",
    "        OVR_results[2,i,:] = np.abs((OVR_results[2,i,:] -1)*1)\n",
    "        OVR_results[3,i,:] = np.abs((OVR_results[3,i,:] -1)*1)\n",
    "        voting_sum =  np.sum(Multi_results[:,i,:], axis=0)+ np.sum((OVR_results[:,i,:]), axis=0)\n",
    "        # print(voting_sum)\n",
    "        # print(voting_results[i])\n",
    "        vote_max = np.where(voting_sum == np.amax(voting_sum.reshape(6)))[0]\n",
    "        if len(vote_max)>1:\n",
    "            vote_max = 6\n",
    "        else:\n",
    "            vote_max = vote_max[0]\n",
    "\n",
    "        voting_results[i] = vote_max\n",
    "\n",
    "\n",
    "        # max = np.where(results[i,:] == np.amin(results[i,:].reshape(6)))[0]\n",
    "        # max2 = np.where(added_results[i,:] == np.amin(added_results[i,:].reshape(6)))[0]\n",
    "        # max3 = np.where(nb_full_results[i,:]== np.amax(nb_full_results[i,:].reshape(6)))[0]\n",
    "        choices = [models_280_choice_, models_200_choice_, models_85_choice_,models_42_choice_]\n",
    "        for x in range(len(choices)):\n",
    "            if len(choices[x])>1:\n",
    "                choices[x] = 6\n",
    "            else:\n",
    "                choices[x] = choices[x][0]\n",
    "            # print(choices)\n",
    "        # print(models_280_choice_)\n",
    "        models_280_choice[i] = choices[0]\n",
    "        models_200_choice[i] = choices[1]\n",
    "        models_85_choice[i] = choices[2]\n",
    "        models_42_choice[i] = choices[3]\n",
    "\n",
    "    with open(\"test_log.txt\", \"a\") as file:\n",
    "        \n",
    "        file.writelines('\\n\\n\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n\\n')\n",
    "\n",
    "        y_pred = models_42_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 42 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 42 Features')\n",
    "        write_results(y, nb_full_42_choice, heading='Multiclass 42 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = models_85_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 85 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 85 Features')\n",
    "        write_results(y, nb_full_42_choice, heading='Multiclass 85 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = models_200_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 200 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 200 Features')\n",
    "        write_results(y, nb_full_200_choice, heading='Multiclass 200 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = models_280_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 280 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 280 Features')\n",
    "        write_results(y, nb_full_280_choice, heading='Multiclass 280 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        write_results(y, voting_results, heading='Voting Ensemble')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_full_280 =  pickle.load(open('models/MulticlassModel_280x360', 'rb'))\n",
    "models_280 =  pickle.load(open('models/EnsembleModels_280x360', 'rb'))\n",
    "nb_full_200 =  pickle.load(open('models/MulticlassModel_200x200', 'rb'))\n",
    "models_200 =  pickle.load(open('models/EnsembleModels_200x200', 'rb'))\n",
    "nb_full_85 =  pickle.load(open('models/MulticlassModel_85x110', 'rb'))\n",
    "models_85 =  pickle.load(open('models/EnsembleModels_85x110', 'rb'))\n",
    "nb_full_42 =  pickle.load(open('models/MulticlassModel_42x65', 'rb'))\n",
    "models_42 =  pickle.load(open('models/EnsembleModels_42x65', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999\r"
     ]
    }
   ],
   "source": [
    "x_280, x_200, x_85, x_42, y = generate_dataset(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_batch(x_280, x_200, x_85, x_42, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0e1bddb7ed37f67f576e4a6a21b2408ebab9a006050dc6d038d31036c144c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
