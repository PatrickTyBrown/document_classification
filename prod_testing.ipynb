{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Style Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import string\n",
    "import random\n",
    "import albumentations as A\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_field(length = 0):\n",
    "    length = length if length else random.randint(2, 20)\n",
    "    field = ''.join(random.choices(string.ascii_letters + string.digits + string.punctuation, k=length))\n",
    "    return field\n",
    "\n",
    "transform = A.Compose([\n",
    "        # A.RandomRotate90(),\n",
    "        # A.Flip(),\n",
    "        # A.Transpose(),\n",
    "        A.ImageCompression(quality_lower=5, p=0.1),\n",
    "        A.OneOf([\n",
    "            # A.IAAAdditiveGaussianNoise(),\n",
    "            A.GaussNoise(p=0.8,var_limit=(0,25)),\n",
    "            # A.ISONoise(p=0.2,),\n",
    "        ], p=0.2),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.MedianBlur(blur_limit=1, p=0.1),\n",
    "            A.Blur(blur_limit=1, p=0.1),\n",
    "        ], p=0.25),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.25, rotate_limit=15, p=0.25),\n",
    "        # A.OneOf([\n",
    "        #     A.OpticalDistortion(p=0.5),\n",
    "        #     A.GridDistortion(p=.5),\n",
    "        #     A.PiecewiseAffine(p=0.5),\n",
    "        # ], p=0.5),\n",
    "        A.OneOf([\n",
    "            A.RandomFog(),\n",
    "            A.RandomRain(),\n",
    "            A.RandomSnow(),\n",
    "            A.RandomSunFlare(),            \n",
    "        ], p=0.1),\n",
    "        # A.HueSaturationValue(p=0.01),\n",
    "        # A.Rotate(66,p=0.3)\n",
    "        # A.ToGray(always_apply=True)\n",
    "    ])\n",
    "# random.seed(42) \n",
    "\n",
    "fonts = [\n",
    "    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_PLAIN,\n",
    "    cv2.FONT_HERSHEY_DUPLEX,\n",
    "    cv2.FONT_HERSHEY_TRIPLEX,\n",
    "    cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX,\n",
    "    cv2.FONT_ITALIC]\n",
    "\n",
    "def generate_target_dictionary():\n",
    "    with open('data_dictionary.json') as data_dict:\n",
    "        categories = json.load(data_dict)['target_data']\n",
    "    return categories\n",
    "\n",
    "template_directory = 'templates_img'\n",
    "text_locations = json.load(open('text_locations.json', 'r'))\n",
    "image_dir =  \"data/\"\n",
    "backgrounds_dir = 'image_backgrounds'\n",
    "\n",
    "categories = generate_target_dictionary()\n",
    "\n",
    "def load_backgrounds():\n",
    "    for filename in os.listdir(backgrounds_dir):\n",
    "        backgrounds = []\n",
    "        img = cv2.imread(backgrounds_dir+ '/'+filename, 1)\n",
    "        backgrounds.append(img)\n",
    "    return backgrounds\n",
    "\n",
    "\n",
    "\n",
    "def agument_image(image, backgrounds, doc_info):\n",
    "    background_img = backgrounds[random.randint(0, len(backgrounds))-1]\n",
    "    img = image\n",
    "    for loc in doc_info:\n",
    "        font = random.choice(fonts)\n",
    "        cv2.putText(img, generate_random_field(),\n",
    "                    (int(loc['x']),int(loc['y'])), font,\n",
    "                    1, (0, 0, 0), 1)\n",
    "\n",
    "    x_size = random.randint(-150,400)\n",
    "    y_size = random.randint(-150,400)\n",
    "    x_size = x_size if x_size > 150 else 0\n",
    "    y_size = y_size if y_size > 150 else 0\n",
    "    x_offset = int(x_size/1.5)\n",
    "    y_offset = int(y_size/1.5)\n",
    "    background_img = cv2.resize(background_img, (850+abs(x_size), 1100+abs(y_size))) \n",
    "\n",
    "    background_img[y_offset:y_offset+img.shape[0], x_offset:x_offset+img.shape[1]] = img\n",
    "    transformed = transform(image=cv2.resize(background_img,(460,720)))\n",
    "    img = transformed['image']\n",
    "\n",
    "    img_280 =cv2.resize(img,(280,360))\n",
    "    img_200 =cv2.resize(img,(200,200))\n",
    "    img_42 = cv2.resize(img,(42,65))\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_280 = cv2.adaptiveThreshold(cv2.cvtColor(img_280, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    img_200 = cv2.adaptiveThreshold(cv2.cvtColor(img_200, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    img_42 = cv2.adaptiveThreshold(cv2.cvtColor(img_42, cv2.COLOR_BGR2GRAY),255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11,2)\n",
    "    return img_280/255, img_200/255, img_42/42\n",
    "\n",
    "dim_size = (280*360)\n",
    "dim_shape = (280,360)\n",
    "\n",
    "def generate_dataset(batch_size):\n",
    "    backgrounds = load_backgrounds()\n",
    "    chunk = batch_size//6\n",
    "\n",
    "    x_280 = np.zeros(shape=(batch_size,(280*360)))\n",
    "    x_200 = np.zeros(shape=(batch_size,(200*200)))\n",
    "    x_42 = np.zeros(shape=(batch_size,(42*65)))\n",
    "\n",
    "    y = np.zeros((batch_size,))\n",
    "    index = 0\n",
    "    #generate our data\n",
    "    for filename in text_locations:\n",
    "        \n",
    "        if text_locations[filename] != {}:\n",
    "            image = cv2.imread(template_directory+ '/'+filename, 1)\n",
    "            image = cv2.resize(image, (850, 1100)) \n",
    "\n",
    "            doc_info = text_locations[filename]\n",
    "            for row in range(chunk):\n",
    "                print(index,end='\\r', flush=True)\n",
    "                img_280, img_200, img_42 = agument_image(image, backgrounds, doc_info)\n",
    "                x_280[index] =  np.reshape(img_280, (280*360))\n",
    "                x_200[index] =  np.reshape(img_200, (200*200))\n",
    "                x_42[index] =  np.reshape(img_42, (42*65))\n",
    "                y[index] = categories[filename[:4]]\n",
    "                index = index + 1\n",
    "\n",
    "    return x_280, x_200, x_42, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our testing functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_batch(x_280, x_200, x_42, y):\n",
    "    # Models\n",
    "    #nb_full_280\n",
    "    # models_280\n",
    "    # nb_full_200\n",
    "    # models_200\n",
    "    # nb_full_42\n",
    "    # models_42\n",
    "\n",
    "    def write_results(y, y_pred, heading):\n",
    "        file.writelines(f'\\n\\n{heading}')\n",
    "        file.writelines(f'\\n\\nData Size:\\t{len(y_pred)}/{len(y_pred)}')\n",
    "        file.writelines(f'\\n\\tAccuracy:\\t{accuracy_score(y, y_pred)}')\n",
    "        file.writelines(f'\\n\\tPrecision:\\t{precision_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tRecall:\\t{recall_score(y, y_pred, average=\"macro\")}')\n",
    "        file.writelines(f'\\n\\tF1:\\t{f1_score(y, y_pred, average=\"macro\")}\\n')\n",
    "        file.writelines(str(confusion_matrix(y, y_pred)))\n",
    "        return True\n",
    "\n",
    "    OVR_results = np.zeros((3,y.shape[0],6))\n",
    "    Multi_results = np.zeros((3, y.shape[0],6))\n",
    "\n",
    "    Multi_results[0] = nb_full_280.predict_proba(x_280)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[0,:,categories[key]] = models_280[key].predict_proba(x_280)[:,0]\n",
    "\n",
    "    Multi_results[1] = nb_full_200.predict_proba(x_200)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[1,:,categories[key]] = models_200[key].predict_proba(x_200)[:,0]\n",
    "\n",
    "    Multi_results[2] = nb_full_42.predict_proba(x_42)[:]\n",
    "    for key in categories.keys():\n",
    "        OVR_results[2,:,categories[key]] = models_200[key].predict_proba(x_200)[:,0]\n",
    "\n",
    "    test = np.copy(y)\n",
    "    test2 = np.copy(y)\n",
    "    test3 = np.copy(y)\n",
    "\n",
    "    nb_full_280_choice = np.copy(y)\n",
    "    models_280_choice = np.copy(y)\n",
    "    nb_full_200_choice = np.copy(y)\n",
    "    models_200_choice = np.copy(y)\n",
    "    nb_full_42_choice = np.copy(y)\n",
    "    models_42_choice = np.copy(y)\n",
    "    voting_sum = np.array((y.shape[0],6))\n",
    "    voting_results = np.copy(y)\n",
    "\n",
    "    for  i in range(y.shape[0]):\n",
    "        # print(i)\n",
    "        # added_results[i,:] = results[i,:] + (np.absolute(nb_full_results[i,:]-1)/1.0e+200)\n",
    "        nb_full_280_choice[i] = np.where(Multi_results[0, i,:] == np.amax(Multi_results[0,i,:].reshape(6)))[0]\n",
    "        models_280_choice_ = np.where(OVR_results[0, i,:] == np.amin(OVR_results[0,i,:].reshape(6)))[0]\n",
    "        nb_full_200_choice[i] = np.where(Multi_results[1, i,:] == np.amax(Multi_results[1,i,:].reshape(6)))[0]\n",
    "        models_200_choice_ = np.where(OVR_results[1, i,:] == np.amin(OVR_results[1,i,:].reshape(6)))[0]\n",
    "        nb_full_42_choice[i] = np.where(Multi_results[2, i,:] == np.amax(Multi_results[2,i,:].reshape(6)))[0]\n",
    "        models_42_choice_ = np.where(OVR_results[2, i,:] == np.amin(OVR_results[2,i,:].reshape(6)))[0]\n",
    "\n",
    "        # print(Multi_results[:,i,:])\n",
    "        # print(np.sum(Multi_results[:,i,:], axis=1))\n",
    "        # print(np.sum(Multi_results[:,i,:], axis=0))\n",
    "        Multi_results[0,i,:] *= 8\n",
    "        Multi_results[1,i,:] *= 6\n",
    "        Multi_results[2,i,:] *= 4\n",
    "        OVR_results[0,i,:] = np.abs((OVR_results[0,i,:] -1)*6)\n",
    "        OVR_results[1,i,:] = np.abs((OVR_results[1,i,:] -1)*1.5)\n",
    "        OVR_results[2,i,:] = np.abs((OVR_results[2,i,:] -1)*1)\n",
    "        voting_sum =  np.sum(Multi_results[:,i,:], axis=0)+ np.sum((OVR_results[:,i,:]), axis=0)\n",
    "        # print(voting_sum)\n",
    "        # print(voting_results[i])\n",
    "        vote_max = np.where(voting_sum == np.amax(voting_sum.reshape(6)))[0]\n",
    "        if len(vote_max)>1:\n",
    "            vote_max = 6\n",
    "        else:\n",
    "            vote_max = vote_max[0]\n",
    "\n",
    "        voting_results[i] = vote_max\n",
    "\n",
    "\n",
    "        # max = np.where(results[i,:] == np.amin(results[i,:].reshape(6)))[0]\n",
    "        # max2 = np.where(added_results[i,:] == np.amin(added_results[i,:].reshape(6)))[0]\n",
    "        # max3 = np.where(nb_full_results[i,:]== np.amax(nb_full_results[i,:].reshape(6)))[0]\n",
    "        choices = [models_280_choice_, models_200_choice_, models_42_choice_]\n",
    "        for x in range(len(choices)):\n",
    "            if len(choices[x])>1:\n",
    "                choices[x] = 6\n",
    "            else:\n",
    "                choices[x] = choices[x][0]\n",
    "            # print(choices)\n",
    "        # print(models_280_choice_)\n",
    "        models_280_choice[i] = choices[0]\n",
    "        models_200_choice[i] = choices[1]\n",
    "        models_42_choice[i] = choices[2]\n",
    "\n",
    "    with open(\"test_log.txt\", \"a\") as file:\n",
    "        \n",
    "        file.writelines('\\n\\n\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n')\n",
    "        file.writelines('*|'*50)\n",
    "        file.writelines('\\n\\n')\n",
    "\n",
    "        y_pred = models_42_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 42 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 42 Features')\n",
    "        write_results(y, nb_full_42_choice, heading='Multiclass 42 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = models_200_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 200 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 200 Features')\n",
    "        write_results(y, nb_full_200_choice, heading='Multiclass 200 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        y_pred = models_280_choice\n",
    "        mask = y_pred != 6\n",
    "        write_results(y, y_pred, heading='Base OVR 280 Features')\n",
    "        write_results(y[mask], y_pred[mask], heading='Filtered OVR 280 Features')\n",
    "        write_results(y, nb_full_280_choice, heading='Multiclass 280 Features')\n",
    "\n",
    "        file.writelines('\\n\\n')\n",
    "        file.writelines(\"===\"*20)\n",
    "\n",
    "        write_results(y, voting_results, heading='Voting Ensemble')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_full_280 =  pickle.load(open('models/MulticlassModel_280x360', 'rb'))\n",
    "models_280 =  pickle.load(open('models/EnsembleModels_280x360', 'rb'))\n",
    "nb_full_200 =  pickle.load(open('models/MulticlassModel_200x200', 'rb'))\n",
    "models_200 =  pickle.load(open('models/EnsembleModels_200x200', 'rb'))\n",
    "nb_full_42 =  pickle.load(open('models/MulticlassModel_42x65', 'rb'))\n",
    "models_42 =  pickle.load(open('models/EnsembleModels_42x65', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399\r"
     ]
    }
   ],
   "source": [
    "x_280, x_200, x_42, y = generate_dataset(2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 40000 features, but BernoulliNB is expecting 9350 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/patrick/classification_project/prod_testing.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_on_batch(x_280, x_200, x_42, y)\n",
      "\u001b[1;32m/home/patrick/classification_project/prod_testing.ipynb Cell 10\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(x_280, x_200, x_42, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m categories\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     OVR_results[\u001b[39m0\u001b[39m,:,categories[key]] \u001b[39m=\u001b[39m models_280[key]\u001b[39m.\u001b[39mpredict_proba(x_280)[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m Multi_results[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m nb_full_200\u001b[39m.\u001b[39;49mpredict_proba(x_200)[:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m categories\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/patrick/classification_project/prod_testing.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     OVR_results[\u001b[39m1\u001b[39m,:,categories[key]] \u001b[39m=\u001b[39m models_200[key]\u001b[39m.\u001b[39mpredict_proba(x_200)[:,\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/naive_bayes.py:125\u001b[0m, in \u001b[0;36m_BaseNB.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_proba\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    110\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return probability estimates for the test vector X.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m        order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_log_proba(X))\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/naive_bayes.py:103\u001b[0m, in \u001b[0;36m_BaseNB.predict_log_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mReturn log-probability estimates for the test vector X.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m    order, as they appear in the attribute :term:`classes_`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X(X)\n\u001b[1;32m    104\u001b[0m jll \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joint_log_likelihood(X)\n\u001b[1;32m    105\u001b[0m \u001b[39m# normalize by P(x) = P(f_1, ..., f_n)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/naive_bayes.py:1147\u001b[0m, in \u001b[0;36mBernoulliNB._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m   1146\u001b[0m     \u001b[39m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1147\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_check_X(X)\n\u001b[1;32m   1148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinarize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m         X \u001b[39m=\u001b[39m binarize(X, threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinarize)\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/naive_bayes.py:519\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_X\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    518\u001b[0m     \u001b[39m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/base.py:580\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    577\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 580\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/classification/lib/python3.10/site-packages/sklearn/base.py:395\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 395\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    396\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 40000 features, but BernoulliNB is expecting 9350 features as input."
     ]
    }
   ],
   "source": [
    "test_on_batch(x_280, x_200, x_42, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f0e1bddb7ed37f67f576e4a6a21b2408ebab9a006050dc6d038d31036c144c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
